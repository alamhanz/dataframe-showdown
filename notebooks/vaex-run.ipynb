{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vaex Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import vaex as vx\n",
    "import time\n",
    "import numpy as np\n",
    "import shutil\n",
    "import gc\n",
    "import joblib\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH_DATA = '../../df-showdown-data/'\n",
    "PATH_DATA = 'O:/df-showdown-data/'\n",
    "TEMP_PATH = PATH_DATA+'temp/'\n",
    "PATH_OUTPUT = '../output/'\n",
    "\n",
    "SLA_NAME = 'pandas-sla.pkl'\n",
    "MEM_NAME = 'pandas-mem.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sla = {}\n",
    "all_memory = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory():\n",
    "    est_memory = psutil.virtual_memory()[3]/1000000000\n",
    "    return est_memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##     Task1 & Task2- I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sla['task1'] = {}\n",
    "all_sla['task1']['csv'] = {}\n",
    "all_sla['task1']['prq'] = {}\n",
    "\n",
    "all_sla['task2'] = {}\n",
    "all_sla['task2']['csv'] = {}\n",
    "all_sla['task2']['prq'] = {}\n",
    "\n",
    "\n",
    "\n",
    "all_memory['task1'] = {}\n",
    "all_memory['task1']['csv'] = {}\n",
    "all_memory['task1']['prq'] = {}\n",
    "\n",
    "all_memory['task2'] = {}\n",
    "all_memory['task2']['csv'] = {}\n",
    "all_memory['task2']['prq'] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error reading csv file O:/df-showdown-data/data_6.csv, write offending chunk to: C:\\Users\\amoza\\AppData\\Local\\Temp\\tmpg7csym6j.csv (len=11527509, first=False, columns=['Order ID'], schema=Order ID: string, encoding=utf8, schema_infer_fraction=0.001).\nPossible causes:\n  * This could be a file encoding error. Consider passing read_options=pyarrow.csv.ReadOptions(encoding=\"ISO-8859-1\") or another encoding as argument.\n  * We might have inferred the wrong schema:\n     * Consider giving a schema hint by e.g. passing read_options=pyarrow.csv.ConvertOptions(column_types={\"SomeId\": pyarrow.string()}).\n     * Consider increasing schema_infer_fraction (e.g. schema_infer_fraction=1 to parse the whole file to infer the schema).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\vaex\\csv.py:215\u001b[0m, in \u001b[0;36mDatasetCsvLazy._read_table\u001b[1;34m(self, data, first, columns)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 215\u001b[0m     table \u001b[39m=\u001b[39m pyarrow\u001b[39m.\u001b[39;49mcsv\u001b[39m.\u001b[39;49mread_csv(file_like, read_options\u001b[39m=\u001b[39;49mread_options, convert_options\u001b[39m=\u001b[39;49mconvert_options)\n\u001b[0;32m    216\u001b[0m \u001b[39mexcept\u001b[39;00m pa\u001b[39m.\u001b[39mArrowInvalid \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\pyarrow\\_csv.pyx:1217\u001b[0m, in \u001b[0;36mpyarrow._csv.read_csv\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\pyarrow\\_csv.pyx:1226\u001b[0m, in \u001b[0;36mpyarrow._csv.read_csv\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\pyarrow\\error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\pyarrow\\error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: CSV parse error: Expected 5 columns, got 7: 406-1465347-1396337,04-03-22,Shipped - \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 ...",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m     15\u001b[0m start_mem \u001b[39m=\u001b[39m get_memory()\n\u001b[1;32m---> 16\u001b[0m dtemp1\u001b[39m.\u001b[39;49mexport(TEMP_PATH \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mtemp.csv\u001b[39;49m\u001b[39m'\u001b[39;49m,index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     17\u001b[0m end_mem \u001b[39m=\u001b[39m get_memory() \u001b[39m-\u001b[39m start_mem\n\u001b[0;32m     18\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\vaex\\dataframe.py:6742\u001b[0m, in \u001b[0;36mDataFrameLocal.export\u001b[1;34m(self, path, progress, chunk_size, parallel, fs_options, fs, **kwargs)\u001b[0m\n\u001b[0;32m   6740\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexport_parquet(path, progress\u001b[39m=\u001b[39mprogress, parallel\u001b[39m=\u001b[39mparallel, chunk_size\u001b[39m=\u001b[39mchunk_size, fs_options\u001b[39m=\u001b[39mfs_options, fs\u001b[39m=\u001b[39mfs)\n\u001b[0;32m   6741\u001b[0m \u001b[39melif\u001b[39;00m naked_path\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m-> 6742\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexport_csv(path, progress\u001b[39m=\u001b[39;49mprogress, parallel\u001b[39m=\u001b[39;49mparallel, chunk_size\u001b[39m=\u001b[39;49mchunk_size, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   6743\u001b[0m \u001b[39melif\u001b[39;00m naked_path\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.json\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m   6744\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexport_json(path, progress\u001b[39m=\u001b[39mprogress, chunk_size\u001b[39m=\u001b[39mchunk_size, parallel\u001b[39m=\u001b[39mparallel, fs_options\u001b[39m=\u001b[39mfs_options, fs\u001b[39m=\u001b[39mfs)\n",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\vaex\\dataframe.py:6983\u001b[0m, in \u001b[0;36mDataFrameLocal.export_csv\u001b[1;34m(self, path, progress, chunk_size, parallel, backend, **kwargs)\u001b[0m\n\u001b[0;32m   6981\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexport_csv_arrow(path, progress\u001b[39m=\u001b[39mprogress, chunk_size\u001b[39m=\u001b[39mchunk_size, parallel\u001b[39m=\u001b[39mparallel, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   6982\u001b[0m \u001b[39melif\u001b[39;00m backend \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpandas\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 6983\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexport_csv_pandas(path, progress\u001b[39m=\u001b[39;49mprogress, chunk_size\u001b[39m=\u001b[39;49mchunk_size, parallel\u001b[39m=\u001b[39;49mparallel, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   6984\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6985\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown backend \u001b[39m\u001b[39m{\u001b[39;00mbackend\u001b[39m}\u001b[39;00m\u001b[39m, should be \u001b[39m\u001b[39m'\u001b[39m\u001b[39marrow\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpandas\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\vaex\\dataframe.py:7040\u001b[0m, in \u001b[0;36mDataFrameLocal.export_csv_pandas\u001b[1;34m(self, path, progress, chunk_size, parallel, **kwargs)\u001b[0m\n\u001b[0;32m   7037\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m   7038\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 7040\u001b[0m \u001b[39mfor\u001b[39;00m i1, i2, chunks \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate_iterator(expressions, chunk_size\u001b[39m=\u001b[39mchunk_size, parallel\u001b[39m=\u001b[39mparallel):\n\u001b[0;32m   7041\u001b[0m     progressbar( i1 \u001b[39m/\u001b[39m n_samples)\n\u001b[0;32m   7042\u001b[0m     chunk_dict \u001b[39m=\u001b[39m {col: values \u001b[39mfor\u001b[39;00m col, values \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(expressions, chunks)}\n",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\vaex\\dataframe.py:3143\u001b[0m, in \u001b[0;36mDataFrame.evaluate_iterator\u001b[1;34m(self, expression, s1, s2, out, selection, filtered, array_type, parallel, chunk_size, prefetch, progress)\u001b[0m\n\u001b[0;32m   3139\u001b[0m previous \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39msubmit(f, previous_i1, previous_i2)\n\u001b[0;32m   3140\u001b[0m \u001b[39mfor\u001b[39;00m l1, l2, i1, i2 \u001b[39min\u001b[39;00m \u001b[39miter\u001b[39m:\n\u001b[0;32m   3141\u001b[0m     \u001b[39m# and we submit the next job before returning the previous, so they run in parallel\u001b[39;00m\n\u001b[0;32m   3142\u001b[0m     \u001b[39m# but make sure the previous is done\u001b[39;00m\n\u001b[1;32m-> 3143\u001b[0m     previous_chunk \u001b[39m=\u001b[39m previous\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m   3144\u001b[0m     current \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39msubmit(f, i1, i2)\n\u001b[0;32m   3145\u001b[0m     \u001b[39myield\u001b[39;00m previous_l1, previous_l2, previous_chunk\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\droid2-1\\lib\\concurrent\\futures\\_base.py:437\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    436\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 437\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\droid2-1\\lib\\concurrent\\futures\\_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    388\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 389\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    390\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    392\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\droid2-1\\lib\\concurrent\\futures\\thread.py:57\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m     59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\vaex\\dataframe.py:3132\u001b[0m, in \u001b[0;36mDataFrame.evaluate_iterator.<locals>.f\u001b[1;34m(i1, i2)\u001b[0m\n\u001b[0;32m   3131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(i1, i2):\n\u001b[1;32m-> 3132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate_implementation(expression, i1\u001b[39m=\u001b[39;49mi1, i2\u001b[39m=\u001b[39;49mi2, out\u001b[39m=\u001b[39;49mout, selection\u001b[39m=\u001b[39;49mselection, filtered\u001b[39m=\u001b[39;49mfiltered, array_type\u001b[39m=\u001b[39;49marray_type, parallel\u001b[39m=\u001b[39;49mparallel, raw\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\vaex\\dataframe.py:6481\u001b[0m, in \u001b[0;36mDataFrameLocal._evaluate_implementation\u001b[1;34m(self, expression, i1, i2, out, selection, filtered, array_type, parallel, chunk_size, raw, progress)\u001b[0m\n\u001b[0;32m   6479\u001b[0m                 arrays[expression] \u001b[39m=\u001b[39m arrays[expression][start:end]\n\u001b[0;32m   6480\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arrays[expression], vaex\u001b[39m.\u001b[39mcolumn\u001b[39m.\u001b[39mColumn):\n\u001b[1;32m-> 6481\u001b[0m                 arrays[expression] \u001b[39m=\u001b[39m arrays[expression][\u001b[39m0\u001b[39;49m:end\u001b[39m-\u001b[39;49mstart]  \u001b[39m# materialize fancy columns (lazy, indexed)\u001b[39;00m\n\u001b[0;32m   6482\u001b[0m             expression_to_evaluate\u001b[39m.\u001b[39mremove(expression_obj)\n\u001b[0;32m   6483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39massign\u001b[39m(thread_index, i1, i2, selection_masks, blocks):\n",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\vaex\\dataset.py:599\u001b[0m, in \u001b[0;36mColumnProxy.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    597\u001b[0m array_chunks \u001b[39m=\u001b[39m []\n\u001b[0;32m    598\u001b[0m ds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mds\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m(item)\n\u001b[1;32m--> 599\u001b[0m \u001b[39mfor\u001b[39;00m chunk_start, chunk_end, chunks \u001b[39min\u001b[39;00m ds\u001b[39m.\u001b[39mchunk_iterator([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname]):\n\u001b[0;32m    600\u001b[0m     ar \u001b[39m=\u001b[39m chunks[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname]\n\u001b[0;32m    601\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ar, pa\u001b[39m.\u001b[39mChunkedArray):\n",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\vaex\\dataset.py:1074\u001b[0m, in \u001b[0;36mDatasetSliced.chunk_iterator\u001b[1;34m(self, columns, chunk_size, reverse)\u001b[0m\n\u001b[0;32m   1073\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchunk_iterator\u001b[39m(\u001b[39mself\u001b[39m, columns, chunk_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, reverse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m-> 1074\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal\u001b[39m.\u001b[39mchunk_iterator(columns, chunk_size\u001b[39m=\u001b[39mchunk_size, reverse\u001b[39m=\u001b[39mreverse, start\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart, end\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mend)\n",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\vaex\\csv.py:410\u001b[0m, in \u001b[0;36mDatasetCsvLazy.chunk_iterator\u001b[1;34m(self, columns, chunk_size, reverse, start, end)\u001b[0m\n\u001b[0;32m    408\u001b[0m     \u001b[39mif\u001b[39;00m column \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_columns:\n\u001b[0;32m    409\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_existence(column)\n\u001b[1;32m--> 410\u001b[0m \u001b[39mfor\u001b[39;00m c1, c2, chunks \u001b[39min\u001b[39;00m filter_none(pwait(buffer(chunk_generator, vaex\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39mmain\u001b[39m.\u001b[39mthread_count_io\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m\u001b[39m+\u001b[39m\u001b[39m3\u001b[39m))):\n\u001b[0;32m    411\u001b[0m     chunks_ready_list\u001b[39m.\u001b[39mappend(chunks)\n\u001b[0;32m    412\u001b[0m     total_row_count \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m([\u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(k\u001b[39m.\u001b[39mvalues())[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m chunks_ready_list])\n",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\vaex\\itertools.py:32\u001b[0m, in \u001b[0;36mfilter_none\u001b[1;34m(i)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfilter_none\u001b[39m(i):\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m i:\n\u001b[0;32m     33\u001b[0m         \u001b[39mif\u001b[39;00m item \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m             \u001b[39myield\u001b[39;00m item\n",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\vaex\\itertools.py:23\u001b[0m, in \u001b[0;36mpwait\u001b[1;34m(i)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpwait\u001b[39m(i):\n\u001b[0;32m     22\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m i:\n\u001b[1;32m---> 23\u001b[0m         \u001b[39myield\u001b[39;00m item\u001b[39m.\u001b[39;49mresult()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\droid2-1\\lib\\concurrent\\futures\\_base.py:444\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    443\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 444\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    445\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    446\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\droid2-1\\lib\\concurrent\\futures\\_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    388\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 389\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    390\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    392\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\droid2-1\\lib\\concurrent\\futures\\thread.py:57\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m     59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\vaex\\csv.py:344\u001b[0m, in \u001b[0;36mDatasetCsvLazy._chunk_producer.<locals>.chunk_reader\u001b[1;34m(reader, first, previous, fragment_info, i)\u001b[0m\n\u001b[0;32m    342\u001b[0m use_threads \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    343\u001b[0m block_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mbytes\u001b[39m)\n\u001b[1;32m--> 344\u001b[0m table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_table(\u001b[39mbytes\u001b[39;49m, first\u001b[39m=\u001b[39;49mi\u001b[39m==\u001b[39;49m\u001b[39m0\u001b[39;49m, columns\u001b[39m=\u001b[39;49mcolumns)\n\u001b[0;32m    346\u001b[0m row_count \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(table)\n\u001b[0;32m    347\u001b[0m row_start \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32md:\\projects\\all_envs\\dfs\\lib\\site-packages\\vaex\\csv.py:220\u001b[0m, in \u001b[0;36mDatasetCsvLazy._read_table\u001b[1;34m(self, data, first, columns)\u001b[0m\n\u001b[0;32m    218\u001b[0m     f \u001b[39m=\u001b[39m tempfile\u001b[39m.\u001b[39mNamedTemporaryFile(mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m, suffix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    219\u001b[0m     f\u001b[39m.\u001b[39mwrite(data)\n\u001b[1;32m--> 220\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    221\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError reading csv file \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath\u001b[39m}\u001b[39;00m\u001b[39m, write offending chunk to: \u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m (len=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m, first=\u001b[39m\u001b[39m{\u001b[39;00mfirst\u001b[39m}\u001b[39;00m\u001b[39m, columns=\u001b[39m\u001b[39m{\u001b[39;00mcolumns\u001b[39m}\u001b[39;00m\u001b[39m, schema=\u001b[39m\u001b[39m{\u001b[39;00mschema\u001b[39m}\u001b[39;00m\u001b[39m, encoding=\u001b[39m\u001b[39m{\u001b[39;00mread_options\u001b[39m.\u001b[39mencoding\u001b[39m}\u001b[39;00m\u001b[39m, schema_infer_fraction=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema_infer_fraction\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    222\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPossible causes:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    223\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m  * This could be a file encoding error. Consider passing read_options=pyarrow.csv.ReadOptions(encoding=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mISO-8859-1\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m) or another encoding as argument.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    224\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m  * We might have inferred the wrong schema:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    225\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m     * Consider giving a schema hint by e.g. passing read_options=pyarrow.csv.ConvertOptions(column_types=\u001b[39m\u001b[39m{\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSomeId\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: pyarrow.string()}).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    226\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m     * Consider increasing schema_infer_fraction (e.g. schema_infer_fraction=1 to parse the whole file to infer the schema).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    227\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[39mreturn\u001b[39;00m table\n",
      "\u001b[1;31mValueError\u001b[0m: Error reading csv file O:/df-showdown-data/data_6.csv, write offending chunk to: C:\\Users\\amoza\\AppData\\Local\\Temp\\tmpg7csym6j.csv (len=11527509, first=False, columns=['Order ID'], schema=Order ID: string, encoding=utf8, schema_infer_fraction=0.001).\nPossible causes:\n  * This could be a file encoding error. Consider passing read_options=pyarrow.csv.ReadOptions(encoding=\"ISO-8859-1\") or another encoding as argument.\n  * We might have inferred the wrong schema:\n     * Consider giving a schema hint by e.g. passing read_options=pyarrow.csv.ConvertOptions(column_types={\"SomeId\": pyarrow.string()}).\n     * Consider increasing schema_infer_fraction (e.g. schema_infer_fraction=1 to parse the whole file to infer the schema).\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "# for i in [6,]:\n",
    "    print(i)\n",
    "    fn = 'data_{}.csv'.format(i)\n",
    "    start = time.perf_counter()\n",
    "    start_mem = get_memory()\n",
    "    dtemp1 = vx.open(PATH_DATA+fn)\n",
    "    end_mem = get_memory() - start_mem\n",
    "    end = time.perf_counter() - start\n",
    "    nrows = str(int(dtemp1.shape[0]/1000000))\n",
    "    all_sla['task1']['csv'][nrows]=end\n",
    "    all_memory['task1']['csv'][nrows]=end_mem\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    start_mem = get_memory()\n",
    "    dtemp1.export(TEMP_PATH + 'temp.csv',index=False)\n",
    "    end_mem = get_memory() - start_mem\n",
    "    end = time.perf_counter() - start\n",
    "    all_sla['task2']['csv'][nrows]=end\n",
    "    all_memory['task2']['csv'][nrows]=end_mem\n",
    "\n",
    "    del dtemp1\n",
    "    gc.collect()\n",
    "    os.remove(TEMP_PATH + 'temp.csv')\n",
    "    time.sleep(2)\n",
    "\n",
    "    fn = 'data_{}.parquet'.format(i)\n",
    "    start = time.perf_counter()\n",
    "    start_mem = get_memory()\n",
    "    dtemp1 = vx.open(PATH_DATA+fn)\n",
    "    end_mem = get_memory() - start_mem\n",
    "    end = time.perf_counter() - start\n",
    "    nrows = str(int(dtemp1.shape[0]/1000000))\n",
    "    all_sla['task1']['prq'][nrows]=end\n",
    "    all_memory['task1']['prq'][nrows]=end_mem\n",
    "    \n",
    "\n",
    "    start = time.perf_counter()\n",
    "    start_mem = get_memory()\n",
    "    dtemp1.export(TEMP_PATH + 'temp.parquet',index=False)\n",
    "    end_mem = get_memory() - start_mem\n",
    "    end = time.perf_counter() - start\n",
    "    all_sla['task2']['prq'][nrows]=end\n",
    "    all_memory['task2']['prq'][nrows]=end_mem\n",
    "\n",
    "    del dtemp1\n",
    "    gc.collect()\n",
    "    os.remove(TEMP_PATH + 'temp.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task1': {'csv': {'0': 0.1597957999999995,\n",
       "   '1': 0.9964031999999996,\n",
       "   '5': 5.668758500000003,\n",
       "   '10': 10.586043699999998,\n",
       "   '25': 27.4135012,\n",
       "   '50': 53.45036569999999},\n",
       "  'prq': {'0': 0.09723499999999952,\n",
       "   '1': 0.3995843000000008,\n",
       "   '5': 1.5912186999999989,\n",
       "   '10': 2.819780399999999,\n",
       "   '25': 6.973793099999995,\n",
       "   '50': 15.728042500000015}},\n",
       " 'task2': {'csv': {'0': 0.5235167999999994,\n",
       "   '1': 3.5646044999999997,\n",
       "   '5': 17.534583899999998,\n",
       "   '10': 34.267026,\n",
       "   '25': 85.98262679999999,\n",
       "   '50': 172.4047499},\n",
       "  'prq': {'0': 0.10737889999999872,\n",
       "   '1': 0.6508976000000004,\n",
       "   '5': 2.8340707999999992,\n",
       "   '10': 5.8789224000000075,\n",
       "   '25': 14.509268200000008,\n",
       "   '50': 28.910167699999988}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task1': {'csv': {'0': 0.013037567999999666,\n",
       "   '1': 0.10687283200000053,\n",
       "   '5': 0.5295185920000005,\n",
       "   '10': 1.1669381120000004,\n",
       "   '25': 2.9038960639999996,\n",
       "   '50': 5.78781184},\n",
       "  'prq': {'0': 0.06413107200000034,\n",
       "   '1': 0.16606412800000037,\n",
       "   '5': 0.5594767360000006,\n",
       "   '10': 0.7485480960000004,\n",
       "   '25': 1.7997946880000004,\n",
       "   '50': 3.669024768}},\n",
       " 'task2': {'csv': {'0': -0.010838015999999229,\n",
       "   '1': -0.016322560000000763,\n",
       "   '5': -0.035409920000001094,\n",
       "   '10': -0.010731520000000216,\n",
       "   '25': -0.01606451199999981,\n",
       "   '50': -0.05526323199999972},\n",
       "  'prq': {'0': 0.028311551999999907,\n",
       "   '1': 0.08388198399999958,\n",
       "   '5': 0.10481254399999962,\n",
       "   '10': 0.09911091200000044,\n",
       "   '25': 0.11252531200000071,\n",
       "   '50': -0.06293504000000105}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data for other task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = [vx.open(PATH_DATA+'data_{}.parquet'.format(i)) for i in [4,5,6,7]]\n",
    "df_right = vx.open(PATH_DATA+'data_to_join.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3 - Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_task = 'task3'\n",
    "all_sla[n_task] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtemp1 in all_df:\n",
    "    start = time.perf_counter()\n",
    "    start_mem = get_memory()\n",
    "    dtemp2 = dtemp1.sort_values(['Date','Amount'],ascending=[True,False])\n",
    "    end_mem = get_memory() - start_mem\n",
    "    end = time.perf_counter() - start\n",
    "    nrows = str(int(dtemp1.shape[0]/1000000))\n",
    "    \n",
    "    all_sla[n_task][nrows]=end\n",
    "    all_memory[n_task][nrows]=end_mem\n",
    "\n",
    "    del dtemp1\n",
    "    del dtemp2\n",
    "    gc.collect()\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3': 0.63470099999995,\n",
       " '9': 2.022421900000154,\n",
       " '19': 4.136806799999931,\n",
       " '34': 10.888667199999873,\n",
       " '45': 16.219327799999974}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sla[n_task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_memory[n_task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task4 - Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_task = 'task4'\n",
    "all_sla[n_task] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtemp1 = all_df[0]\n",
    "filt1 = ['Shipped - Delivered to Buyer', 'Cancelled']\n",
    "for dtemp1 in all_df:\n",
    "    start = time.perf_counter()\n",
    "    start_mem = get_memory()\n",
    "    dtemp2 = dtemp1[(dtemp1['Amount']>300) & (~dtemp1['Status'].isin(filt1))]\n",
    "    end_mem = get_memory() - start_mem\n",
    "    end = time.perf_counter() - start\n",
    "    nrows = str(int(dtemp1.shape[0]/1000000))\n",
    "    \n",
    "    all_sla[n_task][nrows]=end\n",
    "    all_memory[n_task][nrows]=end_mem\n",
    "\n",
    "    del dtemp1\n",
    "    del dtemp2\n",
    "    gc.collect()\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3': 0.21421140000006744,\n",
       " '9': 0.5953160999999909,\n",
       " '19': 1.2111443000001145,\n",
       " '34': 2.2449423999999,\n",
       " '45': 2.9474488999999267}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sla[n_task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_memory[n_task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task5 - Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_task = 'task5'\n",
    "all_sla[n_task] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtemp1 in all_df:\n",
    "    start = time.perf_counter()\n",
    "    start_mem = get_memory()\n",
    "    dtemp2 = pd.merge(dtemp1,df_right,on=['Date','ship-service-level'],how='left')\n",
    "    end_mem = get_memory() - start_mem\n",
    "    end = time.perf_counter() - start\n",
    "    nrows = str(int(dtemp1.shape[0]/1000000))\n",
    "\n",
    "    all_sla[n_task][nrows]=end\n",
    "    all_memory[n_task][nrows]=end_mem\n",
    "\n",
    "    del dtemp1\n",
    "    del dtemp2\n",
    "    gc.collect()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3': 0.5124172000000726,\n",
       " '9': 1.4778619999997318,\n",
       " '19': 2.9784222000002956,\n",
       " '34': 5.352248900000177,\n",
       " '45': 7.045791199999712}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sla[n_task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_memory[n_task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task6 - udf apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_task = 'task6'\n",
    "all_sla[n_task] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    x0 = x + 1\n",
    "    for i in range(25):\n",
    "        if x0<800:\n",
    "            x0 += i\n",
    "            x0 = (x0/3.0)*2.5\n",
    "            x0 = x0*1.2\n",
    "        else:\n",
    "            x0 += i/2.0\n",
    "            x0 = (x0/4.0)*3.8\n",
    "\n",
    "    return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtemp1=all_df[0]\n",
    "# dtemp1['Amount2'] = dtemp1['Amount'].apply(fun)\n",
    "for dtemp1 in all_df:\n",
    "    start = time.perf_counter()\n",
    "    start_mem = get_memory()\n",
    "    dtemp1['Amount2'] = dtemp1['Amount'].apply(fun)\n",
    "    end_mem = get_memory() - start_mem\n",
    "    end = time.perf_counter() - start\n",
    "    nrows = str(int(dtemp1.shape[0]/1000000))\n",
    "    \n",
    "    all_sla[n_task][nrows]=end\n",
    "    all_memory[n_task][nrows]=end_mem\n",
    "    \n",
    "    del dtemp1\n",
    "    gc.collect()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3': 15.072074999999586,\n",
       " '9': 43.90379790000043,\n",
       " '19': 86.94100830000025,\n",
       " '34': 156.61809700000003,\n",
       " '45': 202.96429179999996}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sla[n_task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_memory[n_task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task7 - aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_task = 'task7'\n",
    "all_sla[n_task] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p25(x):\n",
    "    return np.percentile(x,25)\n",
    "\n",
    "def p75(x):\n",
    "    return np.percentile(x,75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtemp1=all_df[0]\n",
    "for dtemp1 in all_df:\n",
    "    start = time.perf_counter()\n",
    "    start_mem = get_memory()\n",
    "    dtemp2 = dtemp1.groupby(['Date','Status']).agg({'Amount':[np.mean, np.size, p25, p75]})\n",
    "    end_mem = get_memory() - start_mem\n",
    "    end = time.perf_counter() - start\n",
    "    nrows = str(int(dtemp1.shape[0]/1000000))\n",
    "    \n",
    "    all_sla[n_task][nrows]=end\n",
    "    all_memory[n_task][nrows]=end_mem\n",
    "\n",
    "    del dtemp1\n",
    "    del dtemp2\n",
    "    gc.collect()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3': 0.787891700000273,\n",
       " '9': 2.2168736999997236,\n",
       " '19': 4.148283800000172,\n",
       " '34': 7.475733299999774,\n",
       " '45': 9.774048400000083}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sla[n_task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_memory[n_task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/pandas-sla.pkl']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(all_sla,PATH_OUTPUT+SLA_NAME)\n",
    "joblib.dump(all_memory,PATH_OUTPUT+MEM_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_sla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfs",
   "language": "python",
   "name": "dfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
